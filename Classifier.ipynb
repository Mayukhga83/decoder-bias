{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cbdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT2_large_PforT_white@T09.json', 'GPT2_large_TforP_white@P09.json', 'GPT2_large_Tfork_white@k50.json', 'GPT2_large_KforT_white@T03.json', 'GPT2_large_PforT_white@T03.json', 'GPT2_large_Tfork_white@k70.json', '.ipynb_checkpoints', 'Toxic_scores', 'GPT2_large_KforT_white@T09.json', 'GPT2_large_TforP_white@P03.json', 'GPT2_large_Tfork_white@k20.json', 'Score.ipynb']\n",
      "['GPT2_large_KforT_white@T03.json', 'GPT2_large_KforT_white@T09.json']\n",
      "['GPT2_large_PforT_white@T09.json', 'GPT2_large_TforP_white@P09.json', 'GPT2_large_Tfork_white@k50.json', 'GPT2_large_PforT_white@T03.json', 'GPT2_large_Tfork_white@k70.json', 'GPT2_large_TforP_white@P03.json', 'GPT2_large_Tfork_white@k20.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "''' For Toxicity \n",
    "    Import the model\n",
    "\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "class ToxicCommentTagger(pl.LightningModule):\n",
    "    def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "        self.n_training_steps = n_training_steps\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.criterion = nn.BCELoss()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "    \n",
    "        output = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "        output = torch.sigmoid(output)    \n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(output, labels)\n",
    "        return loss, output\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, logger=True)\n",
    "        return {\"loss\": loss, \"predictions\": outputs, \"labels\": labels}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"]\n",
    "        loss, outputs = self(input_ids, attention_mask, labels)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "    \n",
    "        labels = []\n",
    "        predictions = []\n",
    "        for output in outputs:\n",
    "            \n",
    "            for out_labels in output[\"labels\"].detach().cpu():\n",
    "                labels.append(out_labels)\n",
    "            for out_predictions in output[\"predictions\"].detach().cpu():\n",
    "                predictions.append(out_predictions)\n",
    "\n",
    "        labels = torch.stack(labels).int()\n",
    "        predictions = torch.stack(predictions)\n",
    "\n",
    "        for i, name in enumerate(LABEL_COLUMNS):\n",
    "            \n",
    "            class_roc_auc = auroc(predictions[:, i], labels[:, i])\n",
    "            self.logger.experiment.add_scalar(f\"{name}_roc_auc/Train\", class_roc_auc, self.current_epoch)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        optimizer = AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "          optimizer,\n",
    "          num_warmup_steps=self.n_warmup_steps,\n",
    "          num_training_steps=self.n_training_steps\n",
    "         )\n",
    "\n",
    "        return dict(\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=dict(\n",
    "        scheduler=scheduler,\n",
    "        interval='step'\n",
    "         )\n",
    "        )\n",
    "\n",
    "df = pd.read_csv(\"/home/mayukh/Generations/toxic_comments.csv\")\n",
    "train_df, val_df = train_test_split(df, test_size=0.05)\n",
    "LABEL_COLUMNS = df.columns.tolist()[2:]\n",
    "\n",
    "####---- could be omitted -----#####\n",
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 12\n",
    "train_toxic = train_df[train_df[LABEL_COLUMNS].sum(axis=1) > 0]\n",
    "train_clean = train_df[train_df[LABEL_COLUMNS].sum(axis=1) == 0]\n",
    "train_df = pd.concat([\n",
    "  train_toxic,\n",
    "  train_clean.sample(15_000)\n",
    "])\n",
    "steps_per_epoch=len(train_df) // BATCH_SIZE\n",
    "total_training_steps = steps_per_epoch * N_EPOCHS\n",
    "warmup_steps = total_training_steps // 5\n",
    "warmup_steps, total_training_steps\n",
    "####-----------------------------#####\n",
    "\n",
    "\n",
    "BERT_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "model = ToxicCommentTagger(\n",
    "  n_classes=len(LABEL_COLUMNS),\n",
    "  n_warmup_steps=warmup_steps,\n",
    "  n_training_steps=total_training_steps  \n",
    "   \n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('/home/mayukh/Generations/model.pth'))\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb5ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def store_toxic_score(prompt_list, data, model, keys = ['0.2', '0.3',  '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']):\n",
    "    '''\n",
    "    Function to store the toxicity ratings scores in a dictionary\n",
    "    '''\n",
    "  keys = keys\n",
    "  out = {}\n",
    "  for item in prompt_list:\n",
    "      out[item] = {}\n",
    "  for i in range(10):\n",
    "    \n",
    "      for item in keys:\n",
    "          lis = []\n",
    "        \n",
    "          for sentence in data[prompt_list[i]][item]:\n",
    "              encoding = tokenizer.encode_plus(\n",
    "                    sentence,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=512,\n",
    "                    return_token_type_ids=False,\n",
    "                    padding=\"max_length\",\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt',\n",
    "              )\n",
    "        \n",
    "              _, test_prediction = model(encoding[\"input_ids\"], encoding[\"attention_mask\"])\n",
    "              test_prediction = test_prediction.flatten().detach().numpy()\n",
    "\n",
    "              lis.append(test_prediction[0])\n",
    "          out[prompt_list[i]][item] = lis\n",
    "        \n",
    "  return out          \n",
    "\n",
    "\n",
    "prompts = [\"Add prompts here as a list of strings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1bc81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "def run_sentiment(prompt_list, data, senti_instance, score_type = 'neg', keys = ['0.2', '0.3',  '0.4', '0.5', '0.6', '0.7', '0.8', '0.9']):\n",
    "    '''\n",
    "    Function to store the sentiment rating \n",
    "    \n",
    "    '''\n",
    "    out = {}\n",
    "    for item in prompt_list:\n",
    "        out[item] = {}\n",
    "    for i in range(10):\n",
    "        \n",
    "        \n",
    "        for item in keys:\n",
    "            \n",
    "            \n",
    "            lis = []\n",
    "        \n",
    "            for sentence in data[prompt_list[i]][item]:\n",
    "                \n",
    "                \n",
    "                \n",
    "                score = senti_instance.polarity_scores(sentence)[score_type]\n",
    "              \n",
    "\n",
    "                lis.append(score)\n",
    "        out[prompt_list[i]][item] = lis\n",
    "        \n",
    "    return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "052ddef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT2_large_KforT_white@T03.json', 'GPT2_large_KforT_white@T09.json']\n",
      "['GPT2_large_PforT@T03.json', 'GPT2_large_PforT@T09.json', 'GPT2_large_Tfork_white@k20.json', 'GPT2_large_Tfork_white@k50.json', 'GPT2_large_Tfork_white@k70.json', 'GPT2_large_TforP@P03.json', 'GPT2_large_TforP@P09.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "arr = os.listdir()\n",
    "print(arr)\n",
    "\n",
    "list_k = []\n",
    "list_other = []\n",
    "for item in arr:\n",
    "    if 'json' in  item.split('.'):\n",
    "        \n",
    "        if 'KforT' in item.split('_'):\n",
    "            list_k.append(item)\n",
    "        else:\n",
    "            list_other.append(item)\n",
    "        \n",
    "        \n",
    "print(list_k)\n",
    "print(list_other)\n",
    "\n",
    "list_k = []\n",
    "list_other = []\n",
    "for item in arr:\n",
    "    if 'json' in  item.split('.'):\n",
    "        \n",
    "        if 'KforT' in item.split('_'):\n",
    "            list_k.append(item)\n",
    "        else:\n",
    "            list_other.append(item)\n",
    "        \n",
    "        \n",
    "print(list_k)\n",
    "print(list_other)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd2e433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89820c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84630b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce3508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7244f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c52aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ad6bae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace95c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
